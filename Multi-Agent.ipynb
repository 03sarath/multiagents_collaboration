{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7884763",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration and Orchestration using LangGraph for Mistral Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a sophisticated multi-agent system built with LangGraph and powered by Mistral. This Multi-Agent City Information System provides comprehensive information about a city, including events, weather, activities, and recommendations.\n",
    "\n",
    "## Goals and Expected Outcomes\n",
    "\n",
    "This notebook aims to demonstrate and achieve the following:\n",
    "\n",
    "1. Integration and Orchestration:\n",
    "   - Showcase the integration of Mistral for building complex LLM applications.\n",
    "   - Demonstrate the power of LangGraph in creating and orchestrating multi-agent systems.\n",
    "   - Illustrate the integration of various data sources and APIs within a multi-agent framework.\n",
    "\n",
    "2. Comprehensive City Information System:\n",
    "   Create a multi-agent system that delivers:\n",
    "   - Upcoming events information:\n",
    "     * Retrieved from a local SQLite database\n",
    "     * Searched online using the Tavily API when local data is unavailable\n",
    "   - Current weather information and analysis using the OpenWeatherMap API\n",
    "   - Suggested activities based on events and weather conditions\n",
    "   - Outfit recommendations considering the weather\n",
    "\n",
    "3. System Flexibility and Adaptability:\n",
    "   - Demonstrate the system's ability to handle varying levels of available information across different cities.\n",
    "   - Showcase the seamless switching between local and online data sources.\n",
    "\n",
    "4. Analysis and Synthesis:\n",
    "   - Provide a final analysis that combines event and weather information into coherent and useful recommendations for city visitors or residents.\n",
    "\n",
    "## Benefits of Using Multi-Agents\n",
    "\n",
    "- **Modularity**: Each agent focuses on a specific task, making the system easier to maintain and extend.\n",
    "- **Flexibility**: Agents can be easily added, removed, or modified without affecting the entire system.\n",
    "- **Scalability**: The system can handle complex workflows by distributing tasks among multiple agents.\n",
    "- **Improved Performance**: Parallel processing of tasks by different agents can lead to faster results.\n",
    "- **Specialization**: Each agent can be optimized for its specific task, improving overall system efficiency.\n",
    "\n",
    "## Why Use LangGraph?\n",
    "\n",
    "LangGraph is a powerful framework for building multi-agent systems:\n",
    "\n",
    "- Provide a structured way to define and manage the flow of information between agents.\n",
    "- Offer built-in support for state management and checkpointing.\n",
    "- Allow for easy visualization of the agent workflow.\n",
    "- Integrate well with large language models and other AI tools.\n",
    "- Support conditional routing, enabling dynamic workflow adjustments based on intermediate results.\n",
    "\n",
    "## System Components\n",
    "\n",
    "Our multi-agent system consists of the following components:\n",
    "\n",
    "1. **EventsDB Tool**: Queries a local SQLite database for event information.\n",
    "2. **Search Tool**: Utilizes the Tavily API to search for online event information when local data is unavailable.\n",
    "3. **Weather Tool**: Fetches current weather data using the OpenWeatherMap API.\n",
    "4. **Analysis Node**: Synthesizes information from other agents to provide comprehensive recommendations.\n",
    "\n",
    "## Key Technologies Used\n",
    "\n",
    "- **Mistral API**: For accessing the Mistral large language model\n",
    "- **Cohere API**: To generate text embeddings of restraunts data\n",
    "- **LangGraph**: For orchestrating the multi-agent system\n",
    "- **Langchain**: For building the RAG pipeline and other LLM interactions\n",
    "- **SQLite**: For local event database storage\n",
    "- **Tavily API**: For online event searches\n",
    "- **OpenWeatherMap API**: For current weather data\n",
    "\n",
    "By the end of this notebook, you'll have a clear understanding of how to build a sophisticated multi-agent system that can process and synthesize information from various sources, adapting to different scenarios and data availability to provide valuable insights about a city.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2796e6e",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    " You'll also need to install the required Python libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce51071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required packages, including openai for embeddings\n",
    "%pip install --upgrade --no-cache-dir --force-reinstall \\\n",
    "    faiss-cpu \\\n",
    "    ipython \\\n",
    "    langchain \\\n",
    "    langchain-community \\\n",
    "    langgraph \\\n",
    "    pandas \\\n",
    "    pydantic \\\n",
    "    pyowm \\\n",
    "    requests \\\n",
    "    tavily-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q mistralai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e031667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph.prebuilt as lp\n",
    "print(dir(lp))             # see what’s in prebuilt\n",
    "import pkgutil\n",
    "print([m.name for m in pkgutil.iter_modules(lp.__path__)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Attempt old import, fall back to the new ToolNode\n",
    "try:\n",
    "    from langgraph.prebuilt import ToolInvocation\n",
    "except ImportError:\n",
    "    from langgraph.prebuilt.tool_node import ToolNode as ToolInvocation\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Environment Setup\n",
    "# Tags: setup\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4d6f1",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "Sign up for the following API keys as they will be used by the agents throughout this notebook:\n",
    "\n",
    "a. **Tavily Search API** is used for performing web searches to find up-to-date information about events in various cities.\n",
    ": \n",
    "   - Visit [Tavily AI](https://tavily.com/) and sign up for an account.\n",
    "   - Once registered, navigate to your dashboard to find your API key.\n",
    "   - Set the environment variable `TAVILY_API_KEY` with your Tavily API key.\n",
    "\n",
    "b. **OpenWeatherMap API** provides current weather data for cities worldwide, which is used in our weather analysis.\n",
    "\n",
    "   - Go to [OpenWeatherMap](https://openweathermap.org/) and create an account.\n",
    "   - After signing up, go to your account dashboard and find your API key.\n",
    "   - Set the environment variable `OPENWEATHERMAP_API_KEY` with your OpenWeatherMap API key.\n",
    "\n",
    "c. **Mistal API** \n",
    "\n",
    "   - Tool Execution: Forces tool calls (tool_choice) for agents (e.g., search_tool, weather_tool).\n",
    "   - Free-Tier Optimization: Uses mistral-tiny with max_tokens=300-600 to stay within limits.\n",
    "   - Analysis Agent: Generates summaries using structured prompts (weather, events, restaurants).\n",
    "   - State Management: Processes JSON responses to update State (e.g., state.analysis_result).\n",
    "   - Set the environment variable `MISTRAL_API_KEY` with your Mistral API key.\n",
    "\n",
    "d. **Cohere API**\n",
    "   - Embeddings: Powers FAISS vector store via CohereEmbeddings (e.g., embed-english-v3.0).\n",
    "   - RAG Queries: Retrieves restaurant data using similarity search (filter={\"stars\": {\"$gte\": 4}}).\n",
    "   - Model Choice: Uses lightweight models (embed-english-light-v2.0) for free-tier efficiency.\n",
    "   - Set the environment variable `COHERE_API_KEY` with your Cohere API key.\n",
    "\n",
    "Now, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Mistral API Integration (Verified Working)\n",
    "# Tags: api, mistral\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def call_mistral_api(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = \"mistral-tiny\",  # Free tier default\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 1000\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Working implementation for Mistral's API endpoint:\n",
    "    POST https://api.mistral.ai/v1/chat/completions\n",
    "    \n",
    "    Returns:\n",
    "        Dict: API response containing:\n",
    "        - choices: [{\"message\": {\"content\": \"response text\"}}]\n",
    "        - usage: {\"total_tokens\": int}\n",
    "    \"\"\"\n",
    "    endpoint = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"***********************THE MISTRAL HEADER***************************\",headers)\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"Mistral API Error: {str(e)}\"\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            error_msg += f\"\\nResponse: {e.response.text}\"\n",
    "        raise RuntimeError(error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain-cohere cohere\n",
    "from langchain_cohere import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Core Setup Cell\n",
    "# Tags: initialization, imports\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "import cohere\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings, ChatCohere  # Changed from OpenAI\n",
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "try:\n",
    "    from langgraph.prebuilt import ToolInvocation\n",
    "except ImportError:\n",
    "    from langgraph.prebuilt.tool_node import ToolNode as ToolInvocation\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Environment variables (Moved to .env - these are just examples)\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"  # Replace with real key or use .env\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MODEL = \"mistral-tiny\"  # Free tier model\n",
    "EMBEDDINGS_MODEL = CohereEmbeddings(\n",
    "    model=\"embed-english-v3.0\",\n",
    "    truncate=\"END\"  # Handles long texts\n",
    ")\n",
    "\n",
    "def call_mistral_api1(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = DEFAULT_MODEL\n",
    ") -> Dict:\n",
    "    \"\"\"Enhanced Mistral API call with error handling\"\"\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            \"https://api.mistral.ai/v1/chat/completions\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.7\n",
    "            },\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            timeout=15  # Added timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Mistral API Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Define Pydantic State model\n",
    "class State(BaseModel):\n",
    "    city: str\n",
    "    messages: List[Dict[str, str]] = Field(default_factory=list)\n",
    "    events_result: str = \"\"\n",
    "    search_result: str = \"\"\n",
    "    weather_info: Dict[str, str] = Field(default_factory=dict)\n",
    "    analysis_result: str = \"\"\n",
    "    restaurant_recommendations: str = \"\"\n",
    "\n",
    "# Verification tests\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Cohere embeddings\n",
    "    test_embed = EMBEDDINGS_MODEL.embed_query(\"test\")\n",
    "    assert len(test_embed) == 1024, \"Cohere embedding dimension mismatch\"\n",
    "    \n",
    "    # Test Mistral connection\n",
    "    test_msg = [{\"role\": \"user\", \"content\": \"Say 'test'\"}]\n",
    "    assert \"test\" in call_mistral_api1(test_msg)[\"choices\"][0][\"message\"][\"content\"].lower()\n",
    "    \n",
    "    logger.info(\"✅ All systems operational\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8aed1a",
   "metadata": {},
   "source": [
    "## Use Case Explanation\n",
    "\n",
    "This system takes a city name as input and provides the following information:\n",
    "\n",
    "1. **Events**: It searches a local database and online sources for upcoming events in the city.\n",
    "2. **Weather**: It fetches current weather information for the city.\n",
    "3. **Activities**: Based on the events and weather, it suggests suitable activities.\n",
    "4. **Outfit Recommendations**: Considering the weather, it provides clothing suggestions.\n",
    "5. **Dining Options**: It recommends highly-rated restaurants in the city.\n",
    "\n",
    "This information is particularly useful for travelers or locals planning their activities.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86054ebc",
   "metadata": {},
   "source": [
    "## Data Preparation and Database Initialization\n",
    "\n",
    "In this section, we'll prepare our data sources and initialize our local database. We're using a SQLite database which is populated with city events data from a JSON file, providing quick access to local event information that ranges from community happenings to cultural events and citywide activities. This database is used by the events_database_tool() for efficient querying and retrieval of city event details, including location, date, and event type. For restaurant recommendations, the generate_restaurants_dataset() function generates synthetic data, creating a custom dataset specifically tailored to our recommendation system. The create_restaurant_vector_store() function processes this data, generates embeddings using Cohere Embeddings, and builds a vector store with Facebook AI Similarity Search (FAISS). (Although this approach is suitable for prototyping, for a more scalable and enterprise-grade solution, we recommend using Amazon Bedrock Knowledge Bases and AWS services).\n",
    "\n",
    "### Event Database Setup\n",
    "\n",
    "Let's load our event data from a JSON file and initialize our SQLite database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572971b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# 1. Load JSON data (with basic error handling)\n",
    "json_path = r'eventsDB_data.json'\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(json_path)\n",
    "    print(f\"Successfully loaded {len(df)} events\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load JSON file\\n{str(e)}\")\n",
    "    exit()  # Stop if we can't load the data\n",
    "\n",
    "# 2. Initialize SQLite database (simplified)\n",
    "db_path = 'local_info.db'\n",
    "\n",
    "try:\n",
    "    # Connect to database (creates if doesn't exist)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Write data to table\n",
    "    df.to_sql('local_events', conn, if_exists='replace', index=False)\n",
    "    print(f\"Created database at {db_path} with {len(df)} events\")\n",
    "    \n",
    "    # Verify\n",
    "    count = pd.read_sql(\"SELECT COUNT(*) FROM local_events\", conn).iloc[0,0]\n",
    "    print(f\"Verification: Database contains {count} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Database operation failed\\n{str(e)}\")\n",
    "finally:\n",
    "    if 'conn' in locals():  # Safely close connection\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eed149",
   "metadata": {},
   "source": [
    "### Text Generation Function\n",
    "\n",
    "This function serves as the core of our agents, allowing them to generate text using the Mistral model..\n",
    "\n",
    "The function works as follows:\n",
    "1. Sends a user message to the Mistral model.\n",
    "2. Invokes the appropriate tool and incorporates the results into the conversation.\n",
    "3. Continues the conversation until a final response is generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2522232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "def generate_text_via_mistral(\n",
    "    model_id: str,\n",
    "    input_text: str,\n",
    "    tool_config: Optional[Dict] = None,\n",
    "    max_iterations: int = 5\n",
    ") -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"\n",
    "    Pure Mistral API implementation with tool handling\n",
    "    Args:\n",
    "        model_id: e.g. \"mistral-tiny\" or \"mistral-large\"\n",
    "        input_text: User query\n",
    "        tool_config: {'tools': [list of available tools]}\n",
    "        max_iterations: Max conversation turns\n",
    "    Returns:\n",
    "        (final_message, last_tool_result)\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "    tool_result = None\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # 1. Call Mistral API\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://api.mistral.ai/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {os.getenv('MISTRAL_API_KEY')}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": model_id,\n",
    "                    \"messages\": messages,\n",
    "                    \"tools\": tool_config.get(\"tools\") if tool_config else None\n",
    "                }\n",
    "            ).json()\n",
    "            \n",
    "            output = response[\"choices\"][0][\"message\"]\n",
    "            messages.append(output)\n",
    "            \n",
    "            # 2. Check if tool needed\n",
    "            if not output.get(\"tool_calls\"):\n",
    "                break\n",
    "                \n",
    "            # 3. Handle tool execution\n",
    "            for tool_call in output[\"tool_calls\"]:\n",
    "                tool_name = tool_call[\"function\"][\"name\"]\n",
    "                tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "                \n",
    "                # Tool router (matches your original tools)\n",
    "                if tool_name == \"get_upcoming_events\":\n",
    "                    tool_result = events_database_tool(tool_args[\"city\"])\n",
    "                elif tool_name == \"get_city_weather\":\n",
    "                    tool_result = weather_tool(tool_args[\"city\"])\n",
    "                elif tool_name == \"search_and_summarize_events\":\n",
    "                    tool_result = search_tool(tool_args[\"city\"])\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "                \n",
    "                # Append tool response\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": json.dumps({\"result\": tool_result})\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    return messages[-1], tool_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b023c5",
   "metadata": {},
   "source": [
    "## Restaurant Vector Database Creation for RAG\n",
    "\n",
    "To enable efficient retrieval of restaurant information, we'll create a vector database using FAISS. This forms the basis of our RAG system for restaurant recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b759c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"COHERE_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa70d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-cohere cohere faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Make sure your Cohere API key is set:\n",
    "# os.environ[\"COHERE_API_KEY\"] = \"your-cohere-api-key\"\n",
    "\n",
    "def create_restaurant_vector_store(df):\n",
    "    # Convert DataFrame to list of dicts\n",
    "    print(\"Converting DataFrame to list of dictionaries...\")\n",
    "    data = df.to_dict(\"records\")\n",
    "\n",
    "    # Split JSON docs\n",
    "    print(\"Splitting documents...\")\n",
    "    splitter = RecursiveJsonSplitter(max_chunk_size=1000)\n",
    "    split_texts, metadata_list = [], []\n",
    "    for item in data:\n",
    "        chunks = splitter.split_text(json_data=item, convert_lists=True)\n",
    "        split_texts.extend(chunks)\n",
    "        meta = {\n",
    "            \"city\": item.get(\"city\", \"\"),\n",
    "            \"stars\": item.get(\"stars\", 0),\n",
    "            \"name\": item.get(\"name\", \"\"),\n",
    "        }\n",
    "        metadata_list.extend([meta] * len(chunks))\n",
    "\n",
    "    # Wrap into LangChain Documents\n",
    "    split_documents = [\n",
    "        Document(page_content=txt, metadata=md)\n",
    "        for txt, md in zip(split_texts, metadata_list)\n",
    "    ]\n",
    "\n",
    "    # Initialize Cohere embeddings\n",
    "    print(\"Initializing Cohere embeddings...\")\n",
    "    embeddings = CohereEmbeddings(\n",
    "        model=\"embed-english-light-v2.0\",    # or another model you have access to\n",
    "        user_agent=\"restaurant-rag\"  # required by LangChain\n",
    "    )\n",
    "\n",
    "    # Build FAISS vector store\n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = FAISS.from_documents(split_documents, embeddings)\n",
    "    num_documents = len(vectorstore.index_to_docstore_id)\n",
    "    \n",
    "    return vectorstore, num_documents\n",
    "\n",
    "# Example usage:\n",
    "df = pd.read_json(r\"restaurant_data.json\")\n",
    "df_cleaned = df.drop_duplicates(subset=[\"name\"], keep=\"first\")\n",
    "restaurant_vectorstore, num_documents = create_restaurant_vector_store(df_cleaned)\n",
    "print(f\"Restaurant Vector Store created successfully.\\nNumber of chunks indexed: {num_documents}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd51b0",
   "metadata": {},
   "source": [
    "### Restaurant Recommendation Function\n",
    "\n",
    "This function uses the RAG system to provide restaurant recommendations. Used by the **Restaurant Recommendation Agent**(`query_restaurants_agent` function):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9653a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohere import Client\n",
    "import numpy as np\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "print(api_key)\n",
    "if not api_key:\n",
    "    raise ValueError(\"COHERE_API_KEY is not set. Please set it in your environment.\")\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_client = Client(api_key)\n",
    "\n",
    "def get_embeddings(texts, model=\"embed-english-light-v2.0\"):\n",
    "    # Get embeddings from Cohere\n",
    "    response = cohere_client.embed(\n",
    "        model=model,\n",
    "        texts=texts\n",
    "    )\n",
    "    return np.array(response.embeddings)  # Return the embeddings as a numpy array\n",
    "\n",
    "def query_restaurants_RAG(city, vectorstore):\n",
    "    query = f\"Find restaurants with 4 stars or higher ratings in {city} and describe their key features\"\n",
    "    print(f\"\\nPerforming query: {query}\")\n",
    "\n",
    "    # Define the filter based on city and star rating\n",
    "    filter_criteria = {\n",
    "        \"stars\": {\"$gte\": 4},  # Filter for ratings >= 4 stars\n",
    "        \"city\": city   # Filter for the specific city (case-insensitive)\n",
    "    }\n",
    "\n",
    "    # Perform similarity search with filter\n",
    "    relevant_docs = vectorstore.similarity_search(query, k=5, filter=filter_criteria)\n",
    "\n",
    "    # Display the relevant documents\n",
    "    if relevant_docs:\n",
    "        print(f\"\\nTop {len(relevant_docs)} relevant documents for high-rated restaurants in {city}:\")\n",
    "        for i, doc in enumerate(relevant_docs, 1):\n",
    "            print(f\"\\nDocument {i}:\")\n",
    "            print(doc.page_content)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    else:\n",
    "        print(f\"No high-rated restaurants found in the vector DB for {city}.\")\n",
    "        context = \"\"\n",
    "\n",
    "    # Generate the prompt for Cohere's text generation\n",
    "    prompt = f\"\"\"Task: Provide information about highly-rated restaurants (4 stars and above) in {city}.\n",
    "\n",
    "Instructions:\n",
    "1. If the following context contains information about restaurants in {city}, describe the key features that contribute to their high ratings. Consider aspects such as cuisine type, ambiance, service, price range, and any standout characteristics.\n",
    "2. Only use the information provided in the context. If certain information is not available, state that it's not mentioned.\n",
    "3. If no information is provided in the context, respond with: \"I'm sorry, but I couldn't find any information about highly-rated restaurants in {city}.\"\n",
    "4. Do not generate or invent any information about restaurants if none is provided in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Please provide your response based on these instructions:\"\"\"\n",
    "\n",
    "        # Generate text using Cohere's Chat API\n",
    "    response = cohere_client.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        message=prompt,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # Extract the response content\n",
    "    response_content = (\n",
    "        response.text.strip()\n",
    "        if hasattr(response, \"text\") and response.text\n",
    "        else f\"I'm sorry, but I couldn't find any information about highly-rated restaurants in {city}.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nGenerated response:\")\n",
    "    print(response_content)\n",
    "    return response_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b2d78",
   "metadata": {},
   "source": [
    "### Testing the Restaurant RAG System\n",
    "\n",
    "While the restaurant dataset may contain more than 5 records for a given city, we set the k value in the similarity search to 5. This limits the number of records returned to a maximum of 5, ensuring the response is concise and focused on the top 5 most relevant records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "city=\"Tampa\"\n",
    "df_cleaned_count = df_cleaned[df_cleaned['city'] == city].shape[0]\n",
    "print(f\"Number of records with {city} as the city in df_cleaned: {df_cleaned_count}\")\n",
    "query_restaurants_RAG(city, restaurant_vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a28bec",
   "metadata": {},
   "source": [
    "### Local Database Query Tool\n",
    "\n",
    "This tool queries the local SQLite database for event information. Used by the **Event Database Agent**(`events_database_agent` function):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_database_tool(city: str) -> str:\n",
    "    \"\"\"Fetch upcoming events for a city from SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        city: City name to search for events\n",
    "    \n",
    "    Returns:\n",
    "        Formatted event strings or 'No events' message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(db_path) as conn:  # Auto-closes connection\n",
    "            query = \"\"\"\n",
    "                SELECT event_name, event_date, description \n",
    "                FROM local_events \n",
    "                WHERE LOWER(city) = LOWER(?)\n",
    "                ORDER BY event_date\n",
    "                LIMIT 3\n",
    "            \"\"\"\n",
    "            df = pd.read_sql_query(query, conn, params=(city,))\n",
    "            \n",
    "            if not df.empty:\n",
    "                print(f\"Found {len(df)} events for {city}:\")\n",
    "                print(df.to_string(index=False))\n",
    "                \n",
    "                events = df.apply(\n",
    "                    lambda row: f\"• {row['event_name']} ({row['event_date']}): {row['description']}\", \n",
    "                    axis=1\n",
    "                )\n",
    "                return \"\\n\".join(events)\n",
    "            else:\n",
    "                return f\"No upcoming events found for {city}.\"\n",
    "                \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        return f\"Error fetching events for {city}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230f218",
   "metadata": {},
   "source": [
    "### Weather Tool\n",
    "\n",
    "This function fetches current weather data for the specified city by calling the . It's used by the **Weather Agent** (`weather_agent()` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2addb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_tool(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches weather data directly from OpenWeatherMap API.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENWEATHERMAP_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENWEATHERMAP_API_KEY not set in environment\")\n",
    "\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        weather_description = data[\"weather\"][0][\"description\"].capitalize()\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        feels_like = data[\"main\"][\"feels_like\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        return (\n",
    "            f\"Weather in {city.title()}:\\n\"\n",
    "            f\"- Condition: {weather_description}\\n\"\n",
    "            f\"- Temperature: {temperature}°C (feels like {feels_like}°C)\\n\"\n",
    "            f\"- Humidity: {humidity}%\"\n",
    "        )\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"⚠️ Error fetching weather for {city}: {str(e)}\"\n",
    "    except (KeyError, IndexError) as e:\n",
    "        return f\"⚠️ Unexpected response format for {city}: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"OWM Key exists:\", \"OPENWEATHERMAP_API_KEY\" in os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(requests.get(\"https://api.openweathermap.org\").status_code)  # Should be 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ece65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_tool(\"Tampa\"))  # Should return weather string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e0e7d",
   "metadata": {},
   "source": [
    "### Online Search Tool\n",
    "\n",
    "This tool performs an online search for events using the Tavily API. Used by the **Search Agent**(`search_agent` function):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6041c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tool(city: str) -> str:\n",
    "    \"\"\"Search for upcoming events in a city using Tavily API\n",
    "    \n",
    "    Args:\n",
    "        city: City name to search events for\n",
    "        \n",
    "    Returns:\n",
    "        Formatted search results with date and time or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize client\n",
    "        client = TavilyClient(api_key=os.environ['TAVILY_API_KEY'])\n",
    "        \n",
    "        # Create focused query\n",
    "        query = f\"Upcoming events, concerts, and festivals in {city} this month\"\n",
    "        \n",
    "        # Get search results\n",
    "        response = client.search(\n",
    "            query=query,\n",
    "            search_depth=\"advanced\",\n",
    "            include_answer=True,\n",
    "            max_results=5  # Limit to top 5 most relevant results\n",
    "        )\n",
    "        \n",
    "        # Format results clearly with Date and Time\n",
    "        if response.get('results'):\n",
    "            formatted_results = []\n",
    "            for i, result in enumerate(response['results'][:5], 1):  # Top 5 only\n",
    "                # Get necessary event info (title, URL, date, time)\n",
    "                title = result.get('title', 'No title')\n",
    "                url = result.get('url', 'No URL')\n",
    "                content = result.get('content', 'No details')\n",
    "                date = result.get('date', 'No date provided')\n",
    "                time = result.get('time', 'No time provided')\n",
    "                \n",
    "                # Format each event\n",
    "                formatted_results.append(\n",
    "                    f\"{i}. {title}\\n\"\n",
    "                    f\"   Date: {date}\\n\"\n",
    "                    f\"   Time: {time}\\n\"\n",
    "                    f\"   {content}\\n\"\n",
    "                    f\"   URL: {url}\\n\"\n",
    "                )\n",
    "            return \"\\n\\n\".join(formatted_results)\n",
    "        else:\n",
    "            return f\"No event information found for {city}\"\n",
    "            \n",
    "    except KeyError:\n",
    "        return \"Tavily API key not found. Please set TAVILY_API_KEY environment variable.\"\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Search failed for {city}: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c893587",
   "metadata": {},
   "source": [
    "## Multi-agent System Definitions\n",
    "\n",
    "Our multi-agent system is composed of several specialized nodes, each responsible for a specific task. These nodes are then organized into a workflow using LangGraph. Here's an overview of the key components:\n",
    "\n",
    "### Agent Functions\n",
    "\n",
    "1. **Events Database Agent (events_database_agent)**: Queries the local database for upcoming events in a specified city.\n",
    "2. **Online Search Agent(search_agent)**: Performs an online search for events when local data is unavailable.\n",
    "3. **Weather Agent(weather_agent)**: Fetches current weather information for the specified city.\n",
    "4. **Restaurants Recommendation Agent(query_restaurants_agent)**: This agent uses our RAG system to provide restaurant recommendations.\n",
    "5. **Analysis Agent(analysis_agent)**: Synthesizes event and weather information to provide recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742b8ea",
   "metadata": {},
   "source": [
    "### 1. Event Database Agent\n",
    "\n",
    "This agent retrieves event information from our local database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_database_agent(state: State) -> State:\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(\"events_database_agent function called\")\n",
    "    print(\"*\" * 50)\n",
    "    \n",
    "    # Directly call your existing tool (no need for Mistral tool negotiation)\n",
    "    try:\n",
    "        events_data = events_database_tool(state.city)\n",
    "        \n",
    "        if \"No upcoming events found\" in events_data:\n",
    "            state.events_result = f\"No upcoming events found for {state.city} in local database.\"\n",
    "        else:\n",
    "            state.events_result = events_data\n",
    "            \n",
    "    except Exception as e:\n",
    "        state.events_result = f\"Error querying local events: {str(e)}\"\n",
    "    \n",
    "    print(f\"Events set to: {state.events_result}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf962b",
   "metadata": {},
   "source": [
    "### 2. Search Agent\n",
    "\n",
    "When local event information is not available in the database, this agent performs an online search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_agent(state: State) -> State:\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(\"search_agent function called\")\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Directly invoke your Tavily-based search tool to fetch raw event details\n",
    "        state.search_result = search_tool(state.city)\n",
    "        \n",
    "        # Check if results are available before summarizing\n",
    "        if state.search_result:\n",
    "            # Summarize the search results using Mistral API\n",
    "            print(\"Summarizing search results...\")\n",
    "            summary = call_mistral_api(\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Summarize these events:\\n\\n{state.search_result}\"\n",
    "                }],\n",
    "                model=\"mistral-small\",\n",
    "                max_tokens=150\n",
    "            )[\"choices\"][0][\"message\"][\"content\"]\n",
    "            state.search_result = summary\n",
    "        else:\n",
    "            state.search_result = f\"No events found for {state.city}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        state.search_result = f\"⚠️ Search failed for {state.city}: {str(e)}\"\n",
    "\n",
    "    # Print the full, multi-line result\n",
    "    print(\"Search result set to:\")\n",
    "    print(state.search_result)\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd9e83",
   "metadata": {},
   "source": [
    "### 3. Weather Agent\n",
    "\n",
    "This agent fetches current weather information for the specified city:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40122d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_agent(state: State) -> State:\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(\"weather_agent function called\")\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Directly call your weather tool (no Mistral API needed for this agent)\n",
    "        weather_data = weather_tool(state.city)\n",
    "        \n",
    "        # Format the result to match your expected state structure\n",
    "        state.weather_info = {\n",
    "            \"city\": state.city,\n",
    "            \"weather\": weather_data if weather_data else \"Weather data not available\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weather tool error: {str(e)}\")\n",
    "        state.weather_info = {\n",
    "            \"city\": state.city,\n",
    "            \"weather\": f\"Error fetching weather: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    print(f\"Weather info set to: {state.weather_info}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90a733",
   "metadata": {},
   "source": [
    "### 4. Restaurant Recommendation Agent\n",
    "\n",
    "This agent uses our RAG system to provide restaurant recommendations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_restaurants_agent(state: State) -> State:\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(\"query_restaurants_agent function called\")\n",
    "    print(\"*\" * 50)\n",
    "    state.restaurant_recommendations = query_restaurants_RAG(state.city, restaurant_vectorstore)\n",
    "    print(f\"Restaurant recommendations set for {state.city}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb75a",
   "metadata": {},
   "source": [
    "### 5. Analysis Agent\n",
    "\n",
    "This agent synthesizes all the gathered information to provide a comprehensive analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_agent(state: State) -> State:\n",
    "    print(\"\\n\" + \"*\" * 50)\n",
    "    print(\"analysis_agent function called\")\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    # Prepare the analysis prompt (unchanged from your original)\n",
    "    prompt = f\"\"\"Analyze the following information about {state.city}:\n",
    "\n",
    "Events from local database: {state.events_result}\n",
    "Events from online search: {state.search_result}\n",
    "Weather: {state.weather_info['weather']}\n",
    "Restaurant Recommendations: {state.restaurant_recommendations}\n",
    "\n",
    "Please provide:\n",
    "1. A brief weather analysis\n",
    "2. Suggested activities based on weather/events\n",
    "3. Outfit recommendations\n",
    "4. Restaurant summary\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Free-tier compatible Mistral call\n",
    "        response = call_mistral_api(\n",
    "            model=\"mistral-tiny\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            max_tokens=600  # Enough for concise analysis\n",
    "        )\n",
    "\n",
    "        # Extract response (matches your original format)\n",
    "        if response.get(\"choices\"):\n",
    "            state.analysis_result = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            state.analysis_result = \"Unable to generate analysis.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis error: {str(e)}\")\n",
    "        state.analysis_result = f\"Analysis unavailable (API error: {str(e)})\"\n",
    "\n",
    "    # Maintain your original message logging\n",
    "    state.messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"text\": state.analysis_result}]\n",
    "    })\n",
    "\n",
    "    print(f\"Analysis result: {state.analysis_result[:500]}...\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4d4fb",
   "metadata": {},
   "source": [
    "## LangGraph Workflow Construction\n",
    "\n",
    "Now that we have defined our agents, let's construct the LangGraph workflow that will orchestrate their interactions. The `build_graph` function creates the workflow structure:\n",
    "\n",
    "1. Adds nodes for each agent (eventsDB, search, weather, restaurant recommendations, and analysis).\n",
    "2. Sets the entry point to the events_database_agent.\n",
    "3. Adds a conditional edge from events_database_agent to either search_agent or weather_agent.\n",
    "4. Connects the remaining nodes in sequence.\n",
    "5. Builds the graph and compiles the workflow.\n",
    "6. Visualize the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf494f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "def build_graph():\n",
    "    workflow = StateGraph(State)\n",
    "\n",
    "    workflow.add_node(\"Events Database Agent\", events_database_agent)\n",
    "    workflow.add_node(\"Online Search Agent\", search_agent)\n",
    "    workflow.add_node(\"Weather Agent\", weather_agent)\n",
    "    workflow.add_node(\"Restaurants Recommendation Agent\", query_restaurants_agent)\n",
    "    workflow.add_node(\"Analysis Agent\", analysis_agent)\n",
    "\n",
    "    workflow.set_entry_point(\"Events Database Agent\")\n",
    "\n",
    "    # Add conditional edge\n",
    "    def route_events(state):\n",
    "        print(f\"Routing events. Current state: {state}\")\n",
    "        print(f\"Events content: '{state.events_result}'\")\n",
    "        if f\"No upcoming events found for {state.city}\" in state.events_result:\n",
    "            print(\"No events found in local DB. Routing to Online Search Agent.\")\n",
    "            return \"Online Search Agent\"\n",
    "        else:\n",
    "            print(\"Events found in local DB. Routing to Weather Agent.\")\n",
    "            return \"Weather Agent\"\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"Events Database Agent\",\n",
    "        route_events,\n",
    "        {\n",
    "            \"Online Search Agent\": \"Online Search Agent\",\n",
    "            \"Weather Agent\": \"Weather Agent\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"Online Search Agent\", \"Weather Agent\")\n",
    "    workflow.add_edge(\"Weather Agent\", \"Restaurants Recommendation Agent\")\n",
    "    workflow.add_edge(\"Restaurants Recommendation Agent\", \"Analysis Agent\")\n",
    "    workflow.add_edge(\"Analysis Agent\", END)\n",
    "    \n",
    "    # Initialize memory to persist state between graph runs\n",
    "    checkpointer = MemorySaver()\n",
    "\n",
    "    # Compile the workflow\n",
    "    app = workflow.compile(checkpointer=checkpointer)\n",
    "    print(\"Workflow compiled successfully\")\n",
    "        \n",
    "    # Visualize the graph\n",
    "    display(\n",
    "        Image(\n",
    "            app.get_graph().draw_mermaid_png(\n",
    "                draw_method=MermaidDrawMethod.API\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #return app and checkpointer\n",
    "    return app, checkpointer\n",
    "\n",
    "print(\"Graph construction function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565688b",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "Finally, let's define our main execution function that will run the entire workflow. The used function orchestrates the entire process:\n",
    "\n",
    "1. Initializes the state with the specified city.\n",
    "2. Streams the events through the workflow.\n",
    "3. Retrieves and displays the final analysis and recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197519b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main(city: str):\n",
    "    try:\n",
    "        print(f\"Starting main execution for city: {city}\")\n",
    "        \n",
    "        # Build the LangGraph workflow\n",
    "        app, checkpointer = build_graph()\n",
    "        print(\"Graph built successfully\")\n",
    "        \n",
    "        # Initialize the input state\n",
    "        initial_state = State(\n",
    "            city=city,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"What's happening in {city} and what should I wear?\"}],\n",
    "            events_result=\"\",\n",
    "            weather_info={},\n",
    "            analysis_result=\"\",\n",
    "            search_result=\"\",\n",
    "            restaurant_recommendations=\"\"\n",
    "        )\n",
    "\n",
    "        # Configuration for streaming execution\n",
    "        config = {\n",
    "            \"recursion_limit\": 150,\n",
    "            \"configurable\": {\"thread_id\": \"42\"}\n",
    "        }\n",
    "\n",
    "        print(\"Starting to stream events through the workflow...\")\n",
    "        events = app.stream(initial_state, config=config)\n",
    "\n",
    "        # Display intermediate outputs\n",
    "        for output in events:\n",
    "            if \"__end__\" not in output:\n",
    "                print(f\"\\nIntermediate output: {output}\")\n",
    "\n",
    "        # Get the final state from checkpointer\n",
    "        final_state_dict = checkpointer.get(config=config)\n",
    "\n",
    "        # Display final assistant message with real line breaks\n",
    "        print(\"\\nFinal response from the assistant:\")\n",
    "        messages = final_state_dict.get('channel_values', {}).get('messages', [])\n",
    "        if messages:\n",
    "            final_message = messages[-1]\n",
    "            if final_message.get(\"role\") == \"assistant\":\n",
    "                content = final_message.get(\"content\")\n",
    "                # If content is a list of dicts with 'text' keys:\n",
    "                if isinstance(content, list):\n",
    "                    for part in content:\n",
    "                        # part may be a dict like {\"text\": \"...\"} or a raw string\n",
    "                        if isinstance(part, dict):\n",
    "                            print(part.get(\"text\", \"\"))\n",
    "                        else:\n",
    "                            print(part)\n",
    "                else:\n",
    "                    # content is a single string\n",
    "                    print(content)\n",
    "            else:\n",
    "                print(\"No assistant message found in final output.\")\n",
    "        else:\n",
    "            print(\"No messages in final state.\")\n",
    "\n",
    "        # Optionally display full final state as JSON\n",
    "        print(\"\\nFinal state from checkpointer:\")\n",
    "        print(json.dumps(final_state_dict, indent=2))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main execution: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "print(\"Main execution function defined.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d82d80",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "Let's run our multi-agent system for three different cities: Tampa, Philadelphia, and New York. Each case demonstrates different aspects of our system's functionality.\n",
    "\n",
    "### Example 1: Tampa, Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77124ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    city = \"Tampa\"\n",
    "    print(f\"Starting script execution for city: {city}\")\n",
    "    main(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d345a74",
   "metadata": {},
   "source": [
    "### Example 2: New York\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28275c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    city = \"New York\"\n",
    "    print(f\"Starting script execution for city: {city}\")\n",
    "    main(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d3dd2",
   "metadata": {},
   "source": [
    "### Example 3: Philadelphia, Pennsylvania\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c02c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    city = \"Philadelphia\"\n",
    "    print(f\"Starting script execution for city: {city}\")\n",
    "    main(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7c3e8",
   "metadata": {},
   "source": [
    "### Analysis of Results\n",
    "\n",
    "1. **Tampa, Florida**:\n",
    "   - Events: Not found in the local database, triggering the search tool.\n",
    "   - Weather: Retrieved from OpenWeatherMap API.\n",
    "   - Restaurants: Recommendations provided through the RAG system.\n",
    "\n",
    "2. **Philadelphia, Pennsylvania**:\n",
    "   - Events: Found in the local database.\n",
    "   - Weather: Retrieved from OpenWeatherMap API.\n",
    "   - Restaurants: Recommendations provided through the RAG system.\n",
    "\n",
    "3. **New York, New York**:\n",
    "   - Events: Found in the local database.\n",
    "   - Weather: Retrieved from OpenWeatherMap API.\n",
    "   - Restaurants: No recommendations available in the RAG system.\n",
    "\n",
    "#### Similarities:\n",
    "- All three cases utilized the weather tool to fetch current weather information.\n",
    "- The analysis node provided recommendations based on available data for each city.\n",
    "\n",
    "#### Differences:\n",
    "- Tampa required the use of the search tool for event information, while Philadelphia and New York used the local database.\n",
    "- Philadelphia had both local event data and restaurant recommendations, providing the most comprehensive results.\n",
    "- New York had local event data but lacked restaurant recommendations, showcasing the system's ability to work with partial information.\n",
    "\n",
    "#### Key Observations:\n",
    "1. The system adapts to available data sources, seamlessly switching between local and online searches for event information.\n",
    "2. Weather information is consistently provided, forming a core part of the recommendations.\n",
    "3. The RAG system for restaurant recommendations enhances the output when data is available but doesn't hinder the overall functionality when it's not.\n",
    "4. The analysis node demonstrates flexibility in synthesizing varying levels of information to provide useful recommendations.\n",
    "\n",
    "This multi-city comparison highlights the robustness and adaptability of our multi-agent system, showcasing its ability to provide valuable insights even when faced with varying levels of available information across different cities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98016ffe",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the power of multi-agent systems built with LangGraph and powered by Mistral. By combining various data sources and agents, we've created a comprehensive city information system that can provide valuable insights for users.\n",
    "\n",
    "Our system successfully gathered and analyzed information about events, weather, activities, and dining options for the specified cities. This showcases the potential of using LangGraph to create complex, multi-step workflows that can process and synthesize information from various sources.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
